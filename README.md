# GPU_benchmarking_toolkit_for_ML
Collection of Machine Learning algorithms for providing benchmarks of GPU hardware.

\begin{abstract}
  
The rapid advancement of machine learning (ML) and artificial intelligence (AI) has sparked a demand for sophisticated computing tools to meet the evolving needs of researchers. In response, we have embarked on a comprehensive ARCC benchmarking toolkit project, aiming to equip researchers with essential resources and tools to enhance their ML/AI projects and push the boundaries of what is achievable in their respective fields. \\

Existing AI benchmarks, such as MLPerf \cite{reddi2020mlperf}, suffer from limited scalability due to fixed problem sizes, while others like AIPerf \cite{9430136} focus on scalability but lack specificity in the ML/AI models used. Additionally, some toolkits, like iMLBench \cite{9305972}, are designed for CPU-GPU integrated architectures, but they have limited workloads and do not emphasize specific ML/AI models. Other commonly used toolkits like Rodina benchmark \cite{5306797}, Mixbench \cite{KONSTANTINIDIS201737} are not built with the focus on AI/ML models. Our benchmarking toolkit is designed explicitly for ML/AI algorithms with a specific emphasis on wboth bare metal (ex: SLURM workload manager) and Kubernetes-based HPC clusters. It offers a diverse range of ML/AI workloads and ensures scalability across different computing platforms, providing researchers with valuable insights into GPU performance under varying scenarios. \\

As the ARCC benchmarking toolkit is designed to be compatible with both bare metal (e.g., SLURM workload manager) and Kubernetes-based high-performance computing (HPC) clusters, accommodating the dataset organization structure of open-source repositories like HuggingFace. It covers a wide range of GPUs, including A10, A30, A40, A100, RTX A6000, V100-32gb, and V100-32Gb, commonly used in ML/AI workloads and representing the state-of-the-art in CUDA-based computing hardware. \\

The toolkit encompasses a variety of ML/AI methods and algorithms, such as natural language processing algorithms (BERT, GPT-2, DNABERT2), image recognition and classification algorithms (YOLO V), text-to-speech algorithms (FastPitch, Coqui-ai TTS, Deep Voice 3), and text-to-image conversion. This diversity allows for an in-depth understanding of each CPU/GPU's performance under different ML/AI workloads. \\

A crucial aspect of the ARCC benchmarking toolkit is its efficient portrayal of system performance data through auto-generated, user-intuitive graphical representations. The toolkit employs advanced statistical analysis to convert raw data into understandable, actionable information. Critical computational metrics such as Central Processing Unit (CPU) utilization, Disk Input/Output Operations Per Second (IOPS), Graphics Processing Unit (GPU) power and core utilization are included in the comprehensive data presentation. The CPU utilization reflects the extent to which the processing power is used, providing insights into potential under-utilization or overload situations. Disk IOPS, on the other hand, gives a detailed account of the speed at which read/write operations are performed, indicative of the hard drive's performance. Additionally, Sequence throughput represents the number of data sequences that can be processed within a specific timeframe, offering an assessment of the application's processing speed. These metrics provide researchers with clear insights into task performance and potential development pipeline bottlenecks.
Each of these metrics provides a unique insight into the performance of the computing environment and the efficiency of the ML/AI tasks being run. By visualizing these metrics, the benchmarking tools provide researchers with a clear and intuitive understanding of how their tasks are performing and where potential bottlenecks may lie in the development pipeline. This understanding can be invaluable in developing proper performance expectations for various model types and in optimizing the performance of their tasks. Additionally, collating and analyzing the aforementioned, our benchmarking suite provides a comprehensive and objective understanding of each GPU's performance characteristics enabling students and researchers to make well-informed decisions when selecting hardware for their specific ML/AI tasks, ensuring optimal utilization of the computing resources. \\

For instance, when examining the Disk IOP performance for GPT-style models, a notable observation emerged during the Docker Container-based test of GPT2. This model was chosen due to its representation of a modern early development unoptimized Language Model (LLM). During the fine-tuning process of GPT2 for three epochs on an NVIDIA A100 GPU, utilizing the OpenWebtext dataset, a consistent decrease in IOPS of approximately 13 percent was identified between each epoch. This observation sheds light on potential bottlenecks within the Data loading pipeline, providing valuable insights that can be used to optimize future workloads. \\
The ARCC benchmarking toolkit is committed to providing unbiased and factual data, ensuring transparency and accuracy in hardware evaluation. It considers emerging technologies, providing insights into potential advancements that may impact ML/AI projects. \\
By empowering researchers with transparent and accurate performance data, the benchmarking suite assists in informed GPU selection for specific algorithms, resulting in influential research outcomes. This initiative reflects our dedication to supporting and advancing ML/AI research, equipping researchers with the necessary tools to drive innovation in the field. 


\end{abstract}
